---
title: "Proyecto Final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
#### Ainhoa Calvo Ejerique

```{r}
library(ggplot2)
library(dplyr)
library(plyr)
```

Con el conjunto de datos diabetes.data

1. Cargar los datos en R.

```{r}
diabetes_conNA <-read.table("diabetes.data", header = TRUE)
sum(is.na(diabetes_conNA))
```

2. Eliminar los missing values, que están codificados como -9999.00.

```{r}
diabetes_conNA2 <-read.table("diabetes.data",header =TRUE, na.strings = "-9999.0")
sum(is.na(diabetes_conNA2))
```

Se han eliminado 10 NA. Y se comprueba que el nuevo df "diabetes" no contiene NA.

```{r}
diabetes <-na.omit(diabetes_conNA2)
sum(is.na(diabetes))
```

3. Ver el tipo de cada una de las variables.

```{r}
str(diabetes)
```

Las variables son de tipo numérico (num e int), con la excepción de SEX que es un factor.

4. Realizar un análisis estadísıstico de las variables: calcular la media, varianza, rangos, etc. ¿Tienen las distintas variables rangos muy diferentes?

Elimino del análisis descriptivo la variable SEX, que es un factor. Utilizo la  función apply() para obtener la media, cuartiles y rango, así como la sd -no la varianza que no es un valor comparable-.

```{r}
dim(diabetes)
apply(diabetes[-2], 2, mean)
apply(diabetes[-2], 2, range)
apply(diabetes[-2], 2, quantile, probs=c(0.25, 0.50, 0.75))
apply(diabetes[-2], 2, sd)
```

Se observan rangos muy diferentes para las distintas variables. 

5. Hacer un gráfico de cajas (boxplot) dónde se pueda ver la información anterior de forma gráfica.

```{r, echo=FALSE}
boxplot(diabetes[-2])
```

6. Calcular la media para las filas que tienen SEX=M y la media para las filas que tienen SEX=F, utilizando la función tapply.

Aplicaré un equivalente que es la utilización de la función split() y a acontinuación lapply

```{r}
s <-split(diabetes, diabetes$SEX)
lapply(s, function(diabetes) colMeans(diabetes[,c(-2)]))
```

7. Calcular la correlación de todas las variables numéricas con la variable Y.

```{r}
corr_y<-cor(diabetes[-2])[,10]
corr_y[-10]
```

La máxima correlación de Y se observa con BMI, y la mínima con S2 y AGE.

8. Realizar un gráfico de dispersión para las variables que tienen más y menos correlación con Y y comentar los resultados. ¿Como sería el gráfico de dispersión entre dos cosas con correlación 1?

```{r, echo=FALSE}
ggplot(diabetes,aes(x=diabetes$BMI, y=diabetes$Y))+geom_line()
ggplot(diabetes,aes(x=diabetes$S2, y=diabetes$Y))+geom_line()
ggplot(diabetes,aes(x=diabetes$AGE, y=diabetes$Y))+geom_line()
```

El gráfico de dispersión entre dos varaibles con correlación 1, sería como el representado abajo, en el que se realiza un gráfico de dispersión entre Y e Y. Es decir, una línea de 45 grados. 

```{r pressure, echo=FALSE}
ggplot(diabetes,aes(x=diabetes$Y, y=diabetes$Y))+geom_line()
```

9. Transformar la variable SEX, que es un factor, en una variable numérica utilizando, por ejemplo, la codificación M=1 y F=2.

```{r}
diabetes_sinfactor<-diabetes %>% mutate(SEXnum = ifelse(SEX=="M", 1, 2))
diabetes_sinfactor <-diabetes_sinfactor %>% select(-c(diabetes_sinfactor$SEX))
```

10. Definimos los outliers como los elementos (filas) de los datos para los que cualquiera de las variables está por encima o por debajo de la mediana más/menos 3 veces el MAD (Median Absolute Deviation). Identificar estos outliers y quitarlos.

En primer lugar, se construye una función para la identificación de outliers, y se utiliza para la identificación de outliers en el df diabetes, del que se exclutye SEX. La función identifica para cada una de las varaibles, la fila que contiene un outlier. En total se identifican 18 outliers.

```{r}
findOutlier <- function(data, corte = 3) {
    ## Calcula la mediana y la mad
    medians <- apply(data, 2, median, na.rm = TRUE)
    madr <- apply(data,2, mad, na.rm=TRUE)
    ## Identifica las celdas que son outliers, con corte por defecto=3
    result <- mapply(function(d, s, r) {
        which(d > s+corte*r|d < s-corte*r)
    }, data, medians,madr)
    result
}

outliers <- findOutlier(diabetes[-2])
outliers
```

En segundo lugar, se eliminan aquellas filas que contienen outliers.

```{r}
removeOutlier <- function(data, outliers) {
    result <- mapply(function(d, o) {
        res <- d
        res[o] <- NA
        return(res)
    }, data, outliers)
    return(as.data.frame(result))
}

dataFilt <- removeOutlier(diabetes[-2], outliers)
dataFilt2 <-na.omit(dataFilt)
```

En total se han eliminado 18 filas.

11. Separar el conjunto de datos en dos, el primero (entrenamiento) conteniendo un 70% de los datos y el segundo (test) un 30%, de forma aleatoria.

```{r}
smp_size <- floor(0.70 * nrow(dataFilt2))
```

Escojo de manera aleatoria los grupos. Fijo una semilla para que la formación de grupos sea reproducible.

```{r}
set.seed(5)
train_s <- sample(seq_len(nrow(dataFilt2)), size=smp_size)
```

```{r}
train <-dataFilt2[train_s, ]
test <-dataFilt2[-train_s, ]
dim(train)
dim(test)
```

12. Escalar los datos para que tengan media 0 y varianza 1, es decir, restar a cada variable numérica su media y dividir por la desviaciónon típica. Calcular la media y desviación en el conjunto de train, y utilizar esa misma media y desviación para escalar el conjunto de test.

En primer lugar, hay que construir un método para normalizar los dos grupos con los parámetros de normalización elegidos (media y desviación estandar de train)

```{r}
normaliza =  function(dF, param){
    as.data.frame(
      Map(function(columna, paramn){
          (columna-paramn[1])/(paramn[2])
    },dF, 
    param)
    )
  }
```

En segundo lugar, hay que obtener los parámetros que se aplicarán en la normalización, es decir, la media y la sd del df train.

```{r}
  param1 = numcolwise(mean) (train)
  param2= numcolwise (sd) (train)
  param =rbind(param1,param2)
```

En tercer lugar, se aplica la función normaliza con los parámetros obtenidos del df train a los dos grupos.

```{r}
train_normalizado = normaliza(train, param)
test_normalizado = normaliza(test, param)
```

En último lugar, se comprueba que el proceso de normalización ha funcinado, mediante el cálculo de medias y sd de cada una de las varaibles, y mediante boxplot.

```{r}
apply(train_normalizado,2,mean)
apply(train_normalizado,2,sd)
apply(test_normalizado,2,mean)
apply(test_normalizado,2,sd)
```

```{r, echo=FALSE}
boxplot(train_normalizado)
```

```{r, echo=FALSE}
boxplot(test_normalizado)
```

13. (Opcional) Realizar un modelo de regresión lineal de la variable de respuesta sobre el resto y ajustarlo por mínimos cuadrados usando únicamente los datos del conjunto de entrenamiento.

```{r}
regresion <- lm(Y~ ., data=train)
summary(regresion)
```

Como puede verse en los p-values tan solo cuatro de los estimadores (el intercept y tres betas, BMI, BP y S5) son significativas. El adjusted R-squared (0.48) no es bueno.

14. (Opcional) Calcular el error cuadrático medio de los datos del conjunto de entrenamiento y de los datos del conjunto de test, definido por la fórmula proporcionada,  donde y es el vector de respuesta de los datos y y sombrero es el vector que predice el modelo (para los mismos datos).

```{r}
y_pred <- predict.lm(regresion, newdata=dataFilt2)
mse <- mean((dataFilt2$Y-y_pred)^2)
cat("Mean squared error:", mse, "\n")
```




